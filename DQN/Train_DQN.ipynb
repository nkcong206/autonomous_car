{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef9ca642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Carrier board is not from a Jetson Developer Kit.\n",
      "WARNNIG: Jetson.GPIO library has not been verified with this carrier board,\n",
      "WARNING: and in fact is unlikely to work correctly.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import serial\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import serial\n",
    "from datetime import datetime\n",
    "from pop import Pilot, AI, LiDAR\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6690fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current coordinate long, lat\n",
    "def getCurrentCoordinate(gps_new = False):\n",
    "    if gps_new==False:\n",
    "        ser = serial.Serial('/dev/ttyUSB1', 9600, timeout=10)\n",
    "        x = ser.readline()\n",
    "        line = x.decode('utf-8', errors='ignore')\n",
    "        if line.find(\"localtion\") != -1:\n",
    "            line = line.replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "            line = line.replace('\"', '')\n",
    "            data = line.split(\":\")[1]\n",
    "            latitude, longitude = float(data.split(\",\")[0]), float(data.split(\",\")[1])\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        serial_port = '/dev/ttyUSB1'\n",
    "        baud_rate = 9600\n",
    "\n",
    "        ser = serial.Serial(serial_port, baud_rate)\n",
    "\n",
    "        while True:\n",
    "            data = ser.readline().decode('utf-8', errors='ignore').strip()\n",
    "            if data.startswith('$GNRMC'):\n",
    "                dataGPS = data.split(',')\n",
    "\n",
    "                if dataGPS[2] == \"A\":\n",
    "                    latgps = float(dataGPS[3])\n",
    "                    if dataGPS[4] == \"S\":\n",
    "                        latgps = -latgps\n",
    "\n",
    "                    latdeg = int(latgps/100)\n",
    "                    latmin = latgps - latdeg*100\n",
    "                    latitude = latdeg + latmin/60\n",
    "\n",
    "                    longps = float(dataGPS[5])\n",
    "                    if dataGPS[6] ==\"W\":\n",
    "                        longps = -longps\n",
    "\n",
    "                    londeg = int(longps/100)\n",
    "                    lonmin = longps - londeg*100\n",
    "                    longitude = londeg + lonmin/60\n",
    "\n",
    "                    if(latitude is not None and longitude is not None):\n",
    "                        return latitude, longitude\n",
    "\n",
    "#Convert distance from (long,lat) to m\n",
    "def computeDistance(lat1, lon1, lat2, lon2):\n",
    "    # Convert degrees to radians\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    R = 6371000  # Approximate radius of the Earth in meters\n",
    "    distance = R * c \n",
    "    return distance\n",
    "\n",
    "#calculate direction between two coordinates\n",
    "def getDirection(lat1, lon1, lat2, lon2):\n",
    "    dlon = lon2 - lon1\n",
    "    lat1 = math.radians(lat1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    y = math.sin(dlon) * math.cos(lat2)\n",
    "    x = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(dlon)\n",
    "    direction = math.atan2(y, x)\n",
    "    direction = math.degrees(direction)\n",
    "    if direction < 0:\n",
    "        direction += 360\n",
    "\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cedff6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarEnv():\n",
    "    def __init__(self):\n",
    "        self.Car = Pilot.AutoCar()\n",
    "        self.lidar = LiDAR.Rplidar()\n",
    "        self.Car.setSensorStatus(euler=1)\n",
    "        self.lidar.connect()\n",
    "        self.lidar.startMotor()\n",
    "        self.obstacle_coef = 0.5\n",
    "        self.lat = 21.048118779800195\n",
    "        self.lon = 105.80128728448946\n",
    "        self.safe_distance = 50\n",
    "        self.l_distance = 0.5\n",
    "        self.time_interval =0.2\n",
    "        self.min_steer = -1\n",
    "        self.max_steer = 1\n",
    "        self.min_forward = -1\n",
    "        self.max_forward = 1\n",
    "    \n",
    "    def setDestination(self, lat, lon):\n",
    "        self.lat = lat\n",
    "        self.lon = lon\n",
    "        \n",
    "        \n",
    "    def get_state(self):\n",
    "        l =[]\n",
    "        compass_direction = self.Car.getEuler('yaw')\n",
    "        lat1, lon1 =getCurrentCoordinate(gps_new = True)\n",
    "        distance = computeDistance(self.lat, self.lon, lat1, lon1)\n",
    "        a_direction = getDirection(lat1, lon1, self.lat, self.lon)\n",
    "        angle = abs(compass_direction - a_direction)* math.pi / 180\n",
    "        \n",
    "        if angle > math.pi:\n",
    "            angle = 2*math.pi - angle\n",
    "        \n",
    "        #get lidar\n",
    "        front_l =[]\n",
    "        behind_l =[]\n",
    "        vectors = self.lidar.getVectors()\n",
    "        for v in vectors:\n",
    "            if 315 <= v[0] and v[0] <= 45:\n",
    "                front_l.append(v[1]/1000)\n",
    "            if 135 <= v[0] and v[0] <= 225:\n",
    "                behind_l.append(v[1]/1000)\n",
    "        min_front = min(front_l) if front_l else 10\n",
    "        min_behind = min(behind_l) if behind_l else 10\n",
    "        return [distance, angle ,min_front , min_behind]\n",
    "    \n",
    "    def compute_reward(self, state, action):\n",
    "        \n",
    "        distance, angle, min_fl, min_bl = state\n",
    "        \n",
    "        #action1: previous speed, positive --> forward, negative --> backward\n",
    "        #direction = int(a/abs(a))  #1 > forward, -1 backward\n",
    "        if action == 1:\n",
    "            direction = -1\n",
    "        else:\n",
    "            direction = 1\n",
    "        reward = 0\n",
    "        if distance ==0:\n",
    "            return math.tanh(1+ math.cos(angle))\n",
    "\n",
    "        reward = math.tanh(1/distance+ math.cos(angle)-self.obstacle_coef*(-1/min_fl+1/min_bl)*direction)\n",
    "\n",
    "        return reward\n",
    "        \n",
    "    def step(self, action, min_fl, min_bl):\n",
    "        \n",
    "        if action == 0:\n",
    "            #let the car move in 0.2s\n",
    "            self.Car.steering = 0\n",
    "            if(min_fl < self.l_distance):\n",
    "                self.Car.stop()\n",
    "            else:\n",
    "                self.Car.forward()\n",
    "                time.sleep(self.time_interval)\n",
    "                self.Car.stop()\n",
    "        elif action ==1:\n",
    "            self.Car.steering = 0\n",
    "            if(min_bl < self.l_distance):\n",
    "                self.Car.stop()\n",
    "            else:\n",
    "                self.Car.backward()\n",
    "                time.sleep(self.time_interval)\n",
    "                self.Car.stop()\n",
    "        elif action == 2:\n",
    "            self.Car.steering = 1\n",
    "            if(min_fl < self.l_distance):\n",
    "                self.Car.stop()\n",
    "            else:\n",
    "                self.Car.forward()\n",
    "                time.sleep(self.time_interval)\n",
    "                self.Car.stop()\n",
    "        elif action == 3:\n",
    "            self.Car.steering = -1\n",
    "            if(min_fl < self.l_distance):\n",
    "                self.Car.stop()\n",
    "            else:\n",
    "                self.Car.forward()\n",
    "                time.sleep(self.time_interval)\n",
    "                self.Car.stop()\n",
    "        else:\n",
    "            self.Car.stop()\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3860275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    " \n",
    "    def _build_model(self):\n",
    "        model = Sequential() \n",
    "        model.add(Dense(32, activation=\"relu\",\n",
    "                        input_dim=self.state_size))\n",
    "        model.add(Dense(32, activation=\"relu\"))\n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\",\n",
    "                     optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    " \n",
    "    def remember(self, state, action, reward, next_state, done): \n",
    "        self.memory.append((state, action,\n",
    "                            reward, next_state, done))\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward # if done \n",
    "        if not done:\n",
    "            target = (reward +\n",
    "                      self.gamma *\n",
    "                      np.amax(self.model.predict(next_state)[0]))\n",
    "                \n",
    "        target_f = self.model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        self.model.fit(state, target_f, epochs=1, verbose=0) \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size) \n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def save(self, name): \n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2223a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CarEnv()\n",
    "state_size = 4\n",
    "batch_size = 32\n",
    "n_episodes = 1000\n",
    "\n",
    "output_dir = \"Data/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "agent = DQNAgent(4, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(n_episodes):\n",
    "    state = env.get_state()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    \n",
    "    done = False\n",
    "    times = 1\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = agent.act(state)\n",
    "        print(action)\n",
    "        \n",
    "        \n",
    "        #next_state, reward, done, _ = env.step(action)\n",
    "        env.step(action, state[0][2], state[0][3])\n",
    "        next_state = env.get_state()\n",
    "        reward = env.compute_reward(next_state, action)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size]) \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if times % 100 == 0:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, n_episodes-1, times, agent.epsilon))\n",
    "            done = True\n",
    "            \n",
    "        times += 1\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.train(batch_size) \n",
    "    if e % 50 == 0:\n",
    "        agent.save(output_dir + \"weights_\"\n",
    "                   + \"{:04d}\".format(e) + \".hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
